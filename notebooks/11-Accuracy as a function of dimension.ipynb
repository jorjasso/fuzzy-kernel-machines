{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "qprc_path = os.path.abspath(os.path.join('..'))\n",
    "if qprc_path not in sys.path:\n",
    "    sys.path.append(qprc_path)\n",
    "    \n",
    "from utils.plots import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import seaborn as sns\n",
    "from  kernelfuzzy.fuzzyset import FuzzySet\n",
    "from kernelfuzzy.fuzzysystem import get_rule_antecedents,plot_membership_fun\n",
    "from  kernelfuzzy.fuzzification import FuzzyData, NonSingletonFuzzifier\n",
    "from kernelfuzzy.kernels import gram_matrix_KBF_kernel,KBFkernel,NonSingletonKernel\n",
    "from sklearn.datasets.samples_generator import make_classification\n",
    "from sklearn.datasets import make_moons, make_circles,make_blobs,load_digits\n",
    "from sklearn.svm import SVC,NuSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from kernelfuzzy.kernels import nonsingleton_gaussian_kernel, gram_matrix_nonsingleton_gaussian_kernel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import skfuzzy as fuzz # for FCM\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from fylearn.nfpc import FuzzyPatternClassifier\n",
    "from fylearn.garules import MultimodalEvolutionaryClassifier\n",
    "from fylearn.fpt import FuzzyPatternTreeTopDownClassifier\n",
    "\n",
    "def NSFS_predict(model, X,X_test, option=0):\n",
    "    #getting best cv parameters\n",
    "    gamma=clf.best_params_['NSKernel__param']\n",
    "    std_proportion=clf.best_params_['Fuzzifier__std_proportion']\n",
    "    best_model=model.best_estimator_['svm']\n",
    "    \n",
    "    if option==0:\n",
    "        ############################\n",
    "        # approach 1:  not embedding the gamma\n",
    "        ############################\n",
    "        print(\"First approach\")\n",
    "        fuzzy_data=NonSingletonFuzzifier( std_proportion=std_proportion, constant_std=True ).transform(X)\n",
    "        fuzzy_data_test=NonSingletonFuzzifier( std_proportion=std_proportion, constant_std=True ).transform(X_test)\n",
    "        #KBF/FBF expansion\n",
    "        K=gram_matrix_KBF_kernel(fuzzy_data_test,fuzzy_data, gamma) \n",
    "        y_pred=(K[:,best_model.support_]@best_model.dual_coef_.T+best_model.intercept_).flatten()\n",
    "        return K, y_pred\n",
    "    \n",
    "    if option==1:\n",
    "        ############################\n",
    "        # approach 2: embedding the gamma\n",
    "        ############################\n",
    "        print(\"Second approach\")\n",
    "        #prediction with a FBF as KBF constructed using the nonsingleton kernel\n",
    "        #nonsingleton fuzzyfication\n",
    "        fuzzy_data=NonSingletonFuzzifier( std_proportion=std_proportion*np.sqrt(1/gamma), constant_std=True ).transform(X)\n",
    "        fuzzy_data_test=NonSingletonFuzzifier( std_proportion=std_proportion*np.sqrt(1/gamma), constant_std=True ).transform(X_test)\n",
    "        #KBF/FBF expansion\n",
    "        K=gram_matrix_KBF_kernel(fuzzy_data_test,fuzzy_data, 1) # gamma=1 because we actually embedded the gamma in the fuzzification and in the rules\n",
    "        y_pred=(K[:,best_model.support_]@best_model.dual_coef_.T+best_model.intercept_).flatten()\n",
    "        return K, y_pred\n",
    "    \n",
    "    if option==2:\n",
    "        ############################\n",
    "        # approach 3: embedding the gamma only in the input and using the rules\n",
    "        ############################\n",
    "        print(\"Third approach I\")\n",
    "        fuzzy_data=NonSingletonFuzzifier( std_proportion=std_proportion, constant_std=True ).transform(X)\n",
    "        rules_antecedents=get_rule_antecedents(best_model, fuzzy_data, gamma)\n",
    "                \n",
    "        fuzzy_data_test=NonSingletonFuzzifier( std_proportion=std_proportion*np.sqrt(1/gamma), constant_std=True ).transform(X_test)\n",
    "        K=gram_matrix_KBF_kernel(fuzzy_data_test,rules_antecedents, 1) # gamma=1 because we actually embedded the gamma in the fuzzification and in the rules\n",
    "        # equivalent to: K=gram_matrix_KBF_kernel(fuzzy_data_test,fuzzy_data[clf.support_], 1) # gamma=1 because we actually embedded the gamma in the fuzzification and in the rules\n",
    "        #print(\"kernel : {}\", K)\n",
    "        y_pred=(K@best_model.dual_coef_.T+best_model.intercept_).flatten()\n",
    "        return K, y_pred\n",
    "    \n",
    "    if option==3:\n",
    "        ############################\n",
    "        # approach 3: embedding the gamma only in the input and using the rules\n",
    "        ############################\n",
    "        print(\"Third approach II\")       \n",
    "        rule_antecedents=NonSingletonFuzzifier( std_proportion=std_proportion*np.sqrt(1/gamma), constant_std=True ).transform(X[best_model.support_, :])\n",
    "        fuzzy_data_test=NonSingletonFuzzifier( std_proportion=std_proportion*np.sqrt(1/gamma), constant_std=True ).transform(X_test)\n",
    "        K=gram_matrix_KBF_kernel(fuzzy_data_test,rule_antecedents, 1) # gamma=1 because we actually embedded the gamma in the fuzzification and in the rules\n",
    "        # equivalent to: K=gram_matrix_KBF_kernel(fuzzy_data_test,fuzzy_data[clf.support_], 1) # gamma=1 because we actually embedded the gamma in the fuzzification and in the rules\n",
    "        #print(\"kernel : {}\", K)\n",
    "        y_pred=(K@best_model.dual_coef_.T+best_model.intercept_).flatten()\n",
    "        return K, y_pred\n",
    "        \n",
    "    if option==4:\n",
    "        ############################\n",
    "        # prediction of a SVM and the non-singleton kernel\n",
    "        ############################\n",
    "        print(\"SVM + nonsingleton kernel\")\n",
    "        fuzzy_data=NonSingletonFuzzifier( std_proportion=std_proportion, constant_std=True ).transform(X)\n",
    "        fuzzy_data_test=NonSingletonFuzzifier( std_proportion=std_proportion, constant_std=True ).transform(X_test)\n",
    "        #KBF/FBF expansion\n",
    "        K=gram_matrix_nonsingleton_gaussian_kernel(fuzzy_data_test,fuzzy_data, gamma) \n",
    "        y_pred=best_model.predict(K)\n",
    "        return K, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook estimates the accuracy of NSFS trained with kernels as a function of data dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension : 2\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First approach\n",
      "Third approach II\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "pipe = Pipeline([\n",
    "    ('Fuzzifier', NonSingletonFuzzifier(constant_std=True)),\n",
    "    ('NSKernel', NonSingletonKernel()),\n",
    "    #('svm', NuSVC(),\n",
    "    ('svm', SVC())])\n",
    "\n",
    "pipe_KBF = Pipeline([\n",
    "    ('Fuzzifier', NonSingletonFuzzifier(constant_std=True)),\n",
    "    ('Kernel', KBFkernel()),\n",
    "    ('svm', SVC())])\n",
    "\n",
    "#noisy train - noisy test\n",
    "#data_dimension=[4,8,16,32,64,128,256,512,1024]\n",
    "data_dimension=range(10,500,50)\n",
    "data_dimension=range(2,20,1)\n",
    "\n",
    "fuzzy_classifiers = (FuzzyPatternClassifier(),\n",
    "                     MultimodalEvolutionaryClassifier(),\n",
    "                     FuzzyPatternTreeTopDownClassifier())\n",
    "\n",
    "\n",
    "list_options_predict=[0,1,2,3,4]\n",
    "acc=[]\n",
    "list_results=[]\n",
    "for d in data_dimension:\n",
    "    print(\"Dimension : {}\".format(d))\n",
    "    #X, y = make_classification(n_samples=1000, n_features=d, n_redundant=0, n_informative=int(90/100*d), random_state=1, n_clusters_per_class=1)\n",
    "    #X_test, y_test = make_classification(n_features=d, n_redundant=0, n_informative=int(90/100*d), random_state=1, n_clusters_per_class=1)\n",
    "    \n",
    "    X_, y_ = make_classification(n_samples=100, n_features=d,\n",
    "                                    n_informative=d, n_redundant=0)\n",
    "\n",
    "    train_samples = 50  # Samples used for training the models\n",
    "\n",
    "    X = X_[:train_samples]\n",
    "    y = y_[:train_samples]\n",
    "\n",
    "    X_test = X_[train_samples:]\n",
    "    y_test = y_[train_samples:]\n",
    "     \n",
    "    #grid search over KBF kernel parameter   \n",
    "    cv_params = dict([\n",
    "        ('Fuzzifier__std_proportion',np.arange(0.01,np.std(X),0.1)),\n",
    "        ('NSKernel__param', 2.0**np.arange(-20,1)),\n",
    "        ('svm__kernel', ['precomputed']),\n",
    "        #('svm__nu', np.arange(0.05,0.1,0.05))\n",
    "        ('svm__C', 2.0**np.arange(-15,15))\n",
    "    ])    \n",
    "    #training\n",
    "    clf = RandomizedSearchCV(pipe, cv_params, cv=5, verbose=1, n_jobs=-1,n_iter=30)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    #predict\n",
    "    #for opt in list_options_predict:\n",
    "    #    K,y_pred=NSFS_predict(clf, X,X_test, option=opt)\n",
    "    #    if opt==0 or opt==1 or opt==4:\n",
    "    #        print(\"acc K\", accuracy_score(y_test, clf.best_estimator_['svm'].predict(K)))\n",
    "    #    print(\"acc y)\", accuracy_score(y_test, sign_fun(y_pred)))\n",
    "    \n",
    "     \n",
    "    results={'dimension':d}\n",
    "    K,y_pred=NSFS_predict(clf, X,X_test, option=0)\n",
    "    results.update({'NSFS' : accuracy_score(y_test, clf.best_estimator_['svm'].predict(K))} )\n",
    "    \n",
    "    #denominator using only the sum of support vectors\n",
    "    K,y_pred=NSFS_predict(clf, X,X_test, option=3)\n",
    "    results.update({'NSFS_den' : accuracy_score(y_test, sign_fun(y_pred))})\n",
    "    \n",
    "    #SVM + nonsingleton kernel\n",
    "    results.update({'SVM_NS' : accuracy_score(y_test, clf.predict(X_test))})\n",
    "        \n",
    " #   print('Model parameters : {}'.format(clf.best_params_))\n",
    " #   print('support vectors: {}'.format( clf.best_estimator_['svm'].support_))\n",
    "    \n",
    "    #getting best cv parameters\n",
    "    gamma=clf.best_params_['NSKernel__param']\n",
    "    std_proportion=clf.best_params_['Fuzzifier__std_proportion']\n",
    "    #print('best parameters gamma, std_proportion, nro_sv ',gamma,std_proportion,clf.best_estimator_['svm'].n_support_)\n",
    "    best_model=clf.best_estimator_['svm']\n",
    "    \n",
    "    results.update({'gamma' : gamma,'std_proportion':std_proportion,'nro_sv':clf.best_estimator_['svm'].n_support_})\n",
    "    \n",
    "    #########################\n",
    "    # SVM + KBF kernel\n",
    "    #########################\n",
    "    cv_params = dict([\n",
    "        ('Fuzzifier__std_proportion',np.arange(0.01,np.std(X),0.1)),\n",
    "        ('Kernel__param', 2.0**np.arange(-20,-1)),\n",
    "        ('svm__kernel', ['precomputed']),\n",
    "        ('svm__C', 2.0**np.arange(-15,15))])\n",
    "    \n",
    "    clf = RandomizedSearchCV(pipe_KBF, cv_params, cv=5, verbose=1, n_jobs=-1,n_iter=30)\n",
    "    clf.fit(X, y)\n",
    "    results.update({'SVM_KBF' : accuracy_score(y_test, clf.predict(X_test))})\n",
    "    results.update({'gamma_KBF' : gamma,'std_proportion_KBF':std_proportion,'nro_sv_KBF':clf.best_estimator_['svm'].n_support_})\n",
    "\n",
    "       \n",
    "    \n",
    "    #a SVM with RBF kernel\n",
    "    # A C-SVM with Gaussian RBF kernel with cross-validation\n",
    "    #########################\n",
    "    #print(\"SVM + Gaussian kernel + cross validation\")\n",
    "    pipe_SVM = Pipeline([('svm', SVC())])\n",
    "\n",
    "    #grid search over KBF kernel parameter   \n",
    "    cv_params_SVM = dict([\n",
    "        ('svm__gamma', 2.0**np.arange(-20,20)),\n",
    "        ('svm__C', 2.0**np.arange(-15,15)),\n",
    "    ])\n",
    "    model_SVM = RandomizedSearchCV(pipe_SVM, cv_params_SVM, cv=5, verbose=1, n_jobs=1,n_iter=30)\n",
    "    model_SVM.fit(X, y)\n",
    "    #print(\"svm_score :\",model_SVM.score(X_test,y_test))\n",
    "    \n",
    "    #KNN\n",
    "    pipe_KNN = Pipeline([('knn', KNeighborsClassifier())])\n",
    "\n",
    "    #grid search over KBF kernel parameter   \n",
    "    cv_params_KNN = dict([\n",
    "        ('knn__n_neighbors', [2,4,8,16,32])\n",
    "    ])\n",
    "    \n",
    "    neigh = RandomizedSearchCV(pipe_KNN, cv_params_KNN, cv=5, verbose=1, n_jobs=1,n_iter=30)\n",
    "    neigh.fit(X, y)\n",
    "    #print(\"knn_score :\",neigh.score(X_test,y_test))\n",
    "    results.update({'svmRBF' : model_SVM.score(X_test,y_test),\n",
    "                    'knn':neigh.score(X_test,y_test)})\n",
    "    \n",
    "    #fuzzy classifiers\n",
    "    for fc in fuzzy_classifiers:\n",
    "        results.update({type(fc).__name__ : accuracy_score(y_test, fc.fit(X, y).predict(X_test))})\n",
    "        \n",
    "        \n",
    "    list_results.append(results)\n",
    "\n",
    "\n",
    "df_results=pd.DataFrame(list_results)\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results.to_csv('df_dimension10_500.csv')\n",
    "#df_results=pd.read_csv('df_dimension10_500.csv')\n",
    "df_results.to_csv('df_dimension2_20.csv')\n",
    "df_results=pd.read_csv('df_dimension2_20.csv')\n",
    "\n",
    "df_results=df_results.rename(columns={\"NSFS\": \"NSFS_NS_DEN_AFTER\", \n",
    "                           \"SVM_KBF\": \"NSFS_SVM_KBF\",\n",
    "                           \"SVM_NS\": \"NSFS_SVM_NS\"})\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "#NSFS using NS kernel and adding the denominator \"after\" training\n",
    "df_results.plot(kind='line',x='dimension',y='NSFS_NS_DEN_AFTER',color='red',style='.-',ax=ax)\n",
    "\n",
    "#NSFS using NS kernel and adding the denominator using only the sv for that \"after\" training (the results seem the same as using all the training samples for estimating the denominator)\n",
    "#df_results.plot(kind='line',x='dimension',y='NSFS_den',color='blue',  ax=ax)\n",
    "\n",
    "#NSFS using KNF kernel direct connection between NSFS and KM\n",
    "df_results.plot(kind='line',x='dimension',y='NSFS_SVM_KBF', color='blue',style='.-', ax=ax)\n",
    "\n",
    "#equivalent to a NSFS without denominator\n",
    "df_results.plot(kind='line',x='dimension',y='NSFS_SVM_NS',  style='.-',ax=ax)\n",
    "\n",
    "#df_results.plot(kind='line',x='dimension',y='knn',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='svmRBF',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='FuzzyPatternClassifier',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='MultimodalEvolutionaryClassifier',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='FuzzyPatternTreeTopDownClassifier',  ax=ax)\n",
    "ax.legend(bbox_to_anchor=(0.5, 0.7))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#df_results.to_csv('df.csv')\n",
    "\n",
    "#######\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "#NSFS using KNF kernel direct connection between NSFS and KM\n",
    "df_results.plot(kind='line',x='dimension',y='NSFS_SVM_KBF', color='blue', style='.-',ax=ax)\n",
    "\n",
    "\n",
    "df_results.plot(kind='line',x='dimension',y='knn', style='.-', ax=ax)\n",
    "df_results.plot(kind='line',x='dimension',y='svmRBF',  style='.-',ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='FuzzyPatternClassifier',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='MultimodalEvolutionaryClassifier',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='FuzzyPatternTreeTopDownClassifier',  ax=ax)\n",
    "ax.legend(bbox_to_anchor=(0.5, 0.7))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#######\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "#NSFS using KNF kernel direct connection between NSFS and KM\n",
    "df_results.plot(kind='line',x='dimension',y='NSFS_SVM_KBF', color='blue',style='.-', ax=ax)\n",
    "\n",
    "\n",
    "#df_results.plot(kind='line',x='dimension',y='knn',  ax=ax)\n",
    "#df_results.plot(kind='line',x='dimension',y='svmRBF',  ax=ax)\n",
    "df_results.plot(kind='line',x='dimension',y='FuzzyPatternClassifier', style='.-', ax=ax)\n",
    "df_results.plot(kind='line',x='dimension',y='MultimodalEvolutionaryClassifier', style='.-', ax=ax)\n",
    "df_results.plot(kind='line',x='dimension',y='FuzzyPatternTreeTopDownClassifier', style='.-', ax=ax)\n",
    "ax.legend(bbox_to_anchor=(0.5, 1.1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#use this\n",
    "#df = pd.DataFrame(np.random.randn(50, 4), \n",
    "#                  index=pd.date_range('1/1/2000', periods=50), columns=list('ABCD'))\n",
    "#df.plot(style='.-')\n",
    "#df.plot(style=['+-','o-','.--','s:'])\n",
    "#df.plot(style='.-', markevery=5)\n",
    "#df.plot(linestyle='-', markevery=100, marker='o', markerfacecolor='black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
